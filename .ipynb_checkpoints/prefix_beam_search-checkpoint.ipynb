{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import json, pickle, os, string, tqdm, kenlm, json\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import groupby\n",
    "import Levenshtein as Lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 = True text\n",
    "#s2 = predicted text\n",
    "\n",
    "def wer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate, defined as the edit distance between the\n",
    "    two provided sentences after tokenizing to words.\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # build mapping of words to integers\n",
    "    b = set(s1.split() + s2.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    # map the words to a char array (Levenshtein packages only accepts\n",
    "    # strings)\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    \n",
    "    return Lev.distance(''.join(w1), ''.join(w2))\n",
    "\n",
    "def cer_(s1, s2):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate, defined as the edit distance.\n",
    "\n",
    "    Arguments:\n",
    "        s1 (string): space-separated sentence\n",
    "        s2 (string): space-separated sentence\n",
    "    \"\"\"\n",
    "    s1, s2, = s1.replace(' ', ''), s2.replace(' ', '')\n",
    "\n",
    "    return Lev.distance(s1, s2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#When using the above implementation, use the code belove to calculate the wer in percentatge: \n",
    "#pred = list of ouput prediction of model (it is the text) # example [\" MY NAME IS HEMANT\", \" I AM A GOD\"]\n",
    "# total_wer = 0\n",
    "# for x in range(len(pred)):\n",
    "#     transcript, reference = data_[x][1], pred[x]\n",
    "#     wer_inst = wer(transcript, reference)\n",
    "#     total_wer += float(wer_inst)\n",
    "# print(\"WER is : \",total_wer/len(pred),\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_best_path(out,labels):\n",
    "    \"implements best path decoding as shown by Graves\"\n",
    "    out = [labels[i] for i in np.argmax(out, axis=1) if i!=labels[-1]]\n",
    "    o = \"\"\n",
    "    for i,j in groupby(out):\n",
    "        o = o + i\n",
    "    return o.replace(\"_\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gred_txt = ctc_best_path(out,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_w = kenlm.LanguageModel('/home/hemant/deep/lm/libri_lm/3-gram.binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using WORD LM\n",
    "def ctc_beam_search(out,labels, prune=0.0001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algo13.043478260869565%'rithm as shown by Graves\"\n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=word age model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "\n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])   \n",
    "                else:\n",
    "                    i_plus = b + c_t\n",
    "                    if len(b) > 0 and c_t == b[-1]: #Extending with the same character as the last one\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif c_t == \" \" and len(b.replace(' ', '')) > 0 : # LM constraints\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = (10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                        \n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][index] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "\n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# beam_txt=ctc_beam_search(out,labels,0.001,k=10,lm=lm_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHARACTER LM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm_c = kenlm.LanguageModel('/home/hemant/decode_humonics/3_char_gram.arpa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_beam(ptot,k):\n",
    "    if len(ptot) < k:\n",
    "        return [i for i in ptot.keys()]\n",
    "    else:\n",
    "        dict_ = sorted(dict((v,k) for k,v in ptot.items()).items(),reverse=True)[:k]\n",
    "        return [i[1] for i in dict_]\n",
    "\n",
    "#using CHARACTER LM\n",
    "def ctc_beam_search_clm(out,labels, prune=0.001, k=20, lm=None,alpha=0.3,beta=12):\n",
    "    \"implements CTC Prefix Search Decoding Algorithm as shown by Graves\"\n",
    "    \n",
    "    '''\n",
    "    out = ctc output\n",
    "    labels = string of labels\n",
    "    prune = prune the ctc output\n",
    "    k=beam-width\n",
    "    lm=charac language model used\n",
    "    alpha,beta = hyper-parameters\n",
    "    '''\n",
    "    \n",
    "    bc_i = 0 # blank/special charatcter index \n",
    "    F = out.shape[1]\n",
    "    out = np.vstack((np.zeros(F), out))\n",
    "    steps = out.shape[0]\n",
    "    \n",
    "    pb, pnb = defaultdict(Counter), defaultdict(Counter)\n",
    "    pb[0][''], pnb[0][''] = 1, 0\n",
    "    prev_beams = ['']\n",
    "    for t in range(1,steps):\n",
    "        pruned_alphabet = [labels[i] for i in np.where(out[t] > prune)[0]]\n",
    "        for b in prev_beams:\n",
    "            for c_t in pruned_alphabet:\n",
    "                index = labels.index(c_t)\n",
    "                #Collapsing case (copy case as the last character in the beam)\n",
    "                if c_t == \"_\": #Extending with a blank\n",
    "                    pb[t][b] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])  \n",
    "                else:  # LM constraints\n",
    "                    i_plus = b + c_t\n",
    "                     #Extending with the same character as the last one\n",
    "                    if len(b) > 0 and c_t == b[-1]:\n",
    "                        pnb[t][b] += out[t][index]*pnb[t-1][b]\n",
    "                        pnb[t][i_plus] += out[t][index]*pb[t-1][b]\n",
    "                    #expanding the beam (extend case as the last character is different)\n",
    "                    elif len(b.replace(' ', '')) > 0 :\n",
    "                        prob = [i[0] for i in lm.full_scores(i_plus,eos=False,bos=False)][-1]\n",
    "                        lm_p = 1#(10**prob)**alpha\n",
    "                        pnb[t][i_plus] += lm_p*out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                    else:\n",
    "                        pnb[t][i_plus] += out[t][index]*(pb[t-1][b] + pnb[t-1][b])\n",
    "                        \n",
    "                    if i_plus not in prev_beams:\n",
    "                        pb[t][i_plus] += out[t][index] * (pb[t - 1][i_plus] + pnb[t - 1][i_plus])\n",
    "                        pnb[t][i_plus] += out[t][index] * pnb[t - 1][i_plus]\n",
    "                        \n",
    "        ptot = pb[t] + pnb[t]\n",
    "        for i in ptot.keys():\n",
    "            ptot[i] = ptot[i]*(len(i)+1)**beta\n",
    "        prev_beams = sort_beam(ptot,k)\n",
    "    return prev_beams[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# beam_txt=ctc_beam_search_clm(out,labels,0.001,k=10,lm=lm_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMANTATION on deepspeech model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hemant/deep/\")\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import os.path\n",
    "from data.data_loader import SpectrogramParser\n",
    "import torch\n",
    "from decoder import GreedyDecoder\n",
    "import argparse\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from opts import add_decoder_args, add_inference_args\n",
    "from utils import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hemant/decode_humonics/updatedAfricanNames/wav_utterance.txt\", \"r\") as f:\n",
    "    data_ = f.readlines()\n",
    "    \n",
    "data_ = [[i.split()[0], \" \".join(i.split()[1:])] for i in data_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = load_model(device, \"/home/hemant/decode_humonics/updatedAfricanNames/deepspeech_final.pth\",False)\n",
    "spect_parser = SpectrogramParser(model.audio_conf, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_w = kenlm.LanguageModel(\"/home/hemant/deep/lm/libri_lm/3-gram.binary\") #word lm\n",
    "lm_c= kenlm.LanguageModel('/home/hemant/2_char_gram.arpa') #character lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use transcribe function for running the code\n",
    "### Chosse the decoding in len number 12-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ is a list containing the audio path \n",
    "\n",
    "def transcribe(audio_path,alpha,beta,k=25,p=0.00001):\n",
    "    start_time = time()\n",
    "    spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "    spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "    spect = spect.to(device)\n",
    "\n",
    "    input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "    out, output_sizes = model(spect, input_sizes)\n",
    "    out = out.cpu().detach().numpy()[0] # gives the corresponding transcription\n",
    "    out = ctc_best_path(out,labels) # greddy decoding\n",
    "#     out = ctc_beam_search(out,labels,p,k,lm=lm_w,alpha,beta) # wordl lm decoding\n",
    "#     out = ctc_beam_search_clm(out,labels,p,k,lm=lm_c,alpha,beta) # character lm decoding\n",
    "\n",
    "    end_time = time()\n",
    "    print(f\"It took {(end_time - start_time)/60} mins\" )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe(audio_path,0.1,11.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To tune the alpha and beta hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_ is a list containing the audio path and the corresponding transcript i.e. \n",
    "#data_[0]= audio path and data_[1] is the corresponding transcript\n",
    "\n",
    "def evalu_(data_,a,b,k=25,p=0.00001):\n",
    "    total_cer, total_wer, num_tokens, num_chars = 0, 0, 0, 0\n",
    "    start_time = time()\n",
    "    for i in data_[:5000]:\n",
    "        audio_path = i[0] \n",
    "        \n",
    "        try:\n",
    "            spect = spect_parser.parse_audio(audio_path).contiguous()\n",
    "            spect = spect.view(1, 1, spect.size(0), spect.size(1))\n",
    "            spect = spect.to(device)\n",
    "\n",
    "            input_sizes = torch.IntTensor([spect.size(3)]).int()\n",
    "            out, output_sizes = model(spect, input_sizes)\n",
    "            out = out.cpu().detach().numpy()[0]\n",
    "            out = ctc_best_path(out,labels) # greddy decoding\n",
    "#             out = ctc_beam_search(out,labels,p,k,lm=lm_w,alpha=a,beta=b) # wordl lm decoding\n",
    "    #         out = ctc_beam_search_clm(out,labels,p,k,lm=lm_c,alpha=a,beta=b) # character lm decoding\n",
    "            transcript, reference = out, i[1]\n",
    "            wer_inst = wer_(transcript, reference)\n",
    "            cer_inst = cer_(transcript, reference)\n",
    "            total_wer += wer_inst\n",
    "            total_cer += cer_inst\n",
    "            num_tokens += len(reference.split())\n",
    "            num_chars += len(reference)\n",
    "\n",
    "        except: pass\n",
    "    wer = float(total_wer) / num_tokens\n",
    "    cer = float(total_cer) / num_chars\n",
    "    print(\"WER :-\",wer*100)\n",
    "    print(\"CER :-:\",cer*100)\n",
    "    end_time = time()\n",
    "    print(f\"It took {(end_time - start_time)/60} mins\" )\n",
    "    return wer*100,cer*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha, beta = np.linspace(0.1,2,20), np.linspace(2,25,20)\n",
    "best = {}\n",
    "for i in alpha:\n",
    "    for j in beta:\n",
    "        print(f\"alpha :- {i} and beta :- {j}\")\n",
    "        wer,cer = evalu_(data_,i,j)\n",
    "        print()\n",
    "        best[wer] = [i,j]\n",
    "        with open(\"best.json\",\"w\") as f: \n",
    "            json.dump(best,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
