---
layout: single
title: Named Entity Recognition(NER) 
excerpt: "Generating NER tags directly through the Speech"
header:
  overlay_color: "#333"
share: false
toc: true

#date:   2019-10-12 16:16:01 -0600


---

## Introduction

 Named entity recognition (NER) is among SLU tasks that usually extract semantic information from textual documents.  NER from speech is made through a pipeline process that consists in processing first an automatic speech recognition (ASR) on the audio and then processing a NER on the ASR outputs. Such approach has some disadvantages (error propa- gation is the prinicipal one ) and it is known that more integrated approaches outperform se- quential ones, when they can be applied. 

 Named entities are sequences of words that bring basic predefined semantic information that usually refers to locations, persons, organization that can be denoted by proper nouns or that are unique in the real world, and they usually include numeric and temporal values. Named entities often constitute the first semantic bricks to extract in order to construct a structured semantic representation of a document content.

+ The Big Picture 
+ Understanding Individual elements of ASR pipeline
+ Handling Out of Vocabulary Words (OOV) words

## The Big Picture
ASR Pipeline contains mainly 2 main components: Acoustic Model and a Language Model. The acoustic model is responsible for the acoustic representation and the language models come into the picture to make sense out of the predicted text. Language model also plays an importatn role when there is ambguity in the predictted text. for example, I <blank> you, if you have to choose between NO and KNOW which one whould you choose?
Ofcourse the second one, but why and how you do that? I would say because you have language understanding as one of your cognitive skills,that's how you  did it.  <br /> 
<br />
let's look at a diagram of the whole pipeline, as shown in Fig. 1:
<p align='center'>
<img src="/assets/images/asr/asr_pipeline.png">
<figcaption align='center'>Fig.1 ASR Pipeline.</figcaption>
</p>


## Understanding Individual elements of ASR pipeline 
### Audio
Input is a RAW Audio signal/waveform, which is just the pressure waves recorded/stored digitally at a certain interval of time, or at a certain frequency. 
### Spectrogram 
It's a better representation of an Audio signal, to fed to a Neural Network.
### Acoustic Model 
Acoustic models calculates/determines the probabiltiy of output(Y) conditioned on an input(X). The input is either Raw audio or spectrogram and the output is the predicted transcription.For each input Acoustic model ouputs probability of a set of pre-defined characters i.e. P(Y/X) 
### Connectionist temporal classification (CTC) 
ctc is a scoring/loss function to calculate the loss of an continous ouput with any Alignment information given. It takes into account all the possible alignments possible for a given output sentence using a Heuristic approach.
### Decoder :
Decoder plays an important role in the ASR pipeline. It helps in getting a word error rate (WER) from 30-40 to under 15 or in some domain specific cases upto under 10 percent (%). The decoding process is of two types:
+ Without the Language model (LM)
+ With the Character level Language model (CLM)



## Handling OOV words
There are Two ways to handle out of vocabulary(OOV) words:
+ Use only the Acoustic model output without any Language Model (LM) and.
+ Use of Character level Language Model.

## References

+ [My paper](https://drive.google.com/open?id=1sMuYV5FV8qaoZ8Q4CLo5HlchO-3Q92WL)

+ [Deepspeech1 paper](https://arxiv.org/pdf/1412.5567.pdf)

+ [Deepspeech2 paper](https://arxiv.org/pdf/1512.02595.pdf)

+ [CTC paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf)

+ [Spectrogram](https://en.wikipedia.org/wiki/Spectrogram)

If I have left someone, please let me know. I willadd the same to the refrence.

## My paper
<iframe src="/assets/images/asr/ASR_BigMM.pdf" width="700" height="800">