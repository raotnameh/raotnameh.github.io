---
layout: single
title: Entropy, Cross-Entropy and  Kullback–Leibler (KL) divergence.
excerpt: "Intution without any complex mathematics"
header:
  overlay_color: "#333"
share: true
toc: true

#date:   2019-12-17 16:16:01 -0600


---
<!-- For inline LaTex, MathJax looks for the \\( ... \\) delimiter. Hence to write a 2 = b 2 as an inline equation, use \\( a^2 = b^2 \\). For displayed equations, the delimiter is either \\[ ... \\]. And so to display a 2 = b 2 + c 2 we would write \\[ a^2 = b^2 + c^2 \\].
 -->

** IN PROGRESS **
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

# Introduction
These three terms may sound different but they are related very closely to each other(on this later). If I have to choose a word which could explain all three terms, it would **INFORMATION** or **UNCERTAINITY**.  
Entropy tells us the degree of information or how uncertain we are about an individual event. While, Cross-Entropy and KL divergence are for more than one event. Two very good explainantion that I found on internet are [post!](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/) and [video!](https://www.youtube.com/watch?v=ErfnhcEV1O8). You may go through them for a deeper understanding.  
A mathematical relation between Entropy, Cross-entropy and KL divergence is: \\[H(p,q) = H(p) + D(p,q) &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;.........eq(1)\\]

Where,  
* H(p,q) =  Cross-Entropy  
* H(p) = Entropy  
* D(p,q) = KL divergence  

## In-depth explainantion
In this section, I will be explaininga each term and build on the previous one to connect the next terms.
### Entropy
Entropy or I would say **_"shanon entropy"_** (as in information theory) is the average rate of information produced from a certain stochastic process. To understand this, let's start with an example.  

In Japan, a tremor occurs at least every five minutes, and each year there are up to 2,000 quakes that can be felt by people. Therefore, the chances of Japan hit by an eartquake are very high compared to Israel. [List of countries by natural disaster risk](https://en.wikipedia.org/wiki/List_of_countries_by_natural_disaster_risk). Now if, somenone tells you that japan got hit by an earthquake you will be less surprised and therefore the amount of information is low in the message. In terms of entropy, look at the figure 1, if probability of an event is either very high or low, then the amount of information in the message/probability distribution is low. Why? well, if you have to guess there are more chances of you being right. 

<p align='center'>
<img src="/assets/images/asr/evsp.jpg">
<figcaption align='center'>Fig.1 Log plot.</figcaption>
</p>

<br />  

In mathematical terms,let's say for an event \\(A\\), the probability of happening is high(**90%**). The amount of information it carries is less. Why? becuase you have already guessed it. To put it in a formula we use \\(\\log\\). why? well it has a nice property. If a number (less than 1, as in the case of probabilitys) is less it's (-ve)output is high and vice-versa. In the Fig. 1 you can see clearly the phenomena.


<p align='center'>
<img src="/assets/images/asr/log.jpg">
<figcaption align='center'>Fig.2 Log plot.</figcaption>
</p>

<br /> 

Therefore Uncertainity can be defined as \\[-log(A)\\]
But, in Machine Learning (ML) we use Expectation of an event therefore multiply it with the probability of the event. Therefore, Expectation of uncertainity is:  \\[-A * log(A)\\]
Expectation of uncertainity is **Entropy** or The  expected Information content from the event is **Entropy** .

### Cross-Entropy and KL divergence.
I'm gonna define the individual terms first and then will explaing them in detail.  

+ Cross-Entropy = It measures the distance or similarity between two Probability distribution.  
+ KL divergence = Information gain of a given when we move from distribution **q** to **p**.  

**p** = true probability distribution for a given data.  
**q** = Approximate probability distribution for the given data.  
So, if we start with some distribution **q** and try to move to **p**. Then, we are gaining information related to data true distibution and it is known as KL divergence.
<font size="2"> **side note**: if p and q are equal means Entropy and Cross-Entropy are equal. </font>  
<br />

Before moving ahead, we need to know one last thing i.e. Likelihood ratio (LR).  

If value of some random variable **X** is sampled from some unkown distribution, the likelihood ratio (LR) expresses how much more likely the sample has come from distribution **p** than from distribution **q**. We can define this mathematically using likelihood ratio or **LR** for short. \\[LR = \frac{p(x)}{q(x)}\\]  

 
It's for one draw. For drawing **n** independetn samples from a distribution, the LR for n samples become. \\[LR =\prod\limits_{i=1}^n \frac{p(x_i)}{q(x_i)}\\]

and in log space (just so the multiplication does not vanish or explode). \\[LR = \sum\limits_{i=1}^n \log (\frac{p(x_i)}{q(x_i)})\\]

So now we have the likelihood ratio as a sum. Let’s say we want to answer the question of how much, on average, each sample gives evidence of  \\(p(x)\\)over \\(q(x)\\). To do this, we can take the expected value of the likelihood ratio and arrive at: \\[D(p,q) = \sum\limits_{i=1}^n p(x_i) * \log (\frac{p(x_i)}{q(x_i)})\\]


This expresion is the defination of **KL divergence**, i.e. how much the data is from distribution p rather than q. 

Simplifying the above expression gives us: \\[D(p,q) = \sum\limits_{i=1}^n p(x_i) * \log p(x_i) - \sum\limits_{i=1}^n p(x_i) * \log q(x_i)\\]  

When compare to _eqation 1_ **<font size="3">Cross-Entropy </font>** is: \\[H(p,q) = \sum\limits_{i=1}^n p(x_i) * \log q(x_i)\\]   

Since KL divergence is gain or loss of information when we move between probability distribution p and q. Let's say we start we some random distribution **q** then then value of KL divergence will be very low

Now, we have all the terms:  **<font size='4'> Entropy, Cross-Entropy and KL divergence. </font>**

## End
I hope you guys enjoyed this, please feel free to leave a comment or reach to me on **raotnameh@gmail.com**