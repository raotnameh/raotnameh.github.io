---
layout: single
title: Entropy, Cross-Entropy and  Kullback–Leibler (KL) divergence.
excerpt: "Intution without any complex math"
header:
  overlay_color: "#333"
share: true
toc: true

#date:   2019-12-17 16:16:01 -0600


---
<!-- For inline LaTex, MathJax looks for the \\( ... \\) delimiter. Hence to write a 2 = b 2 as an inline equation, use \\( a^2 = b^2 \\). For displayed equations, the delimiter is either \\[ ... \\]. And so to display a 2 = b 2 + c 2 we would write \\[ a^2 = b^2 + c^2 \\].
 -->


** IN PROGRESS **
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

# Introduction
These three terms may sound different but they are related very closely to each other(on this later). If I have to choose a word which could explain all three terms, it would **INFORMATION** or **UNCERTAINITY**.  
Entropy tells us the degree of information about an individual event. While, Cross-Entropy and KL divergence for more than one event. Two of very good explainantion that i found on internet are [post!](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/) and [video!](https://www.youtube.com/watch?v=ErfnhcEV1O8).

## In-depth explainantion
In this section, I will be explaininga each term and build on the previous one to connect the next.
### Entropy
Entropy or I would say shanon entropy (as in information theory) is the average rate of information produced from a certain stochastic process. To understand this, let's start with an example.  

In Japan: A tremor occurs at least every five minutes, and each year there are up to 2,000 quakes that can be felt by people. Therefore, the chances of Japan(155) gettng stuck by an eartquake are very high compared to Israel(15) ([*list of countries by natural disaster risk*]
(https://en.wikipedia.org/wiki/List_of_countries_by_natural_disaster_risk)). Now if, somenone tells you that japan got hit by an earthquake you will be less surprised and therefore the amount of information is low compared to if israel hit by an earthquake you will be surprised and in return you are getting more information.  

In mathemtical terms, for an event \\(A\\), the probability of happening is high(**90%**) then there is less information comapred to if it's low(**10%**). To put it in a formula we use log. why? well it has a nice property if a number (less than 1, as in the case of probabilitys) is low it's output is high and vice-versa.  

Therefore Uncertainity can be defined as \\(-log(A)\\). But, in Machine Learning (ML) we use Expectation of an event therefore multiply it with the probability of the event. Therefore, Expectation of uncertainity is:  \\(-A * log(A)\\). Expectation of uncertainity is Entropy or Entropy is average Information that we get, when a random sample is drawn from a given event.

### Cross-Entropy and KL divergence.
Cross-Entropy = Way of measuring the distance or similarity between two distribution and in the ML case Probability distribution.  
KL divergence = Information gained when we move from distribution **q** to **p**. <br />
**p** = true probability distribution for a given data.  
**q** = Approximate probability distribution for the given data.
<font size="2"> **_Note_**: if p and q are equal means Entropy and Cross-Entropy are equal. </font>
Relation between Entropy, Cross-entropy and KL divergence is: \\[H(p,q) = H(p) + (D<sub>KL</sub>(p,q))\\]

Where,  
* H(p,q) =  Cross-Entropy  
* H(p) = Entropy  
* D<sub>KL</sub>(p,q) = KL divergence  

If value of x is sampled from some unkown distribution, the likelihood ratio (LR) expresses how much more likely the sample has come from distribution **p** than from distribution **q**. Since KL divergence is gain or loss of information when we move between probability distribution p and q. We can define this mathematically using likelihood ratio or **LR** for short. \\[LR = \frac{p(x)}{q(x)}\\]  

 
It's for one draw. For drawing **n** independetn samples from a distribution LR becomes. \\[LR =\prod\limits_{i=1}^n \frac{p(x_i)}{q(x_i)}\\]

and in log space. \\[LR = \sum\limits_{i=1}^n \frac{p(x_i)}{q(x_i)}\\]

So now we have the likelihood ratio as a summation. Let’s say we want to answer the question of how much, on average, each sample gives evidence of  \\(p(x)\\)over \\(q(x)\\). To do this, we can take the expected value of the likelihood ratio and arrive at: \\[D(p,q) = \sum\limits_{i=1}^n p(x) * \frac{p(x_i)}{q(x_i)}\\]]