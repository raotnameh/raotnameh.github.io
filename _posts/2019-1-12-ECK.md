---
layout: single
title: Entropy, Cross-Entropy and  Kullbackâ€“Leibler (KL) divergence.
excerpt: "Intution without any complex math"
header:
  overlay_color: "#333"
share: true
toc: true

#date:   2019-1-12 16:16:01 -0600


---

These three terms may sound different but they are related very closely to each other(on this later). If I have to choose a word which could explain all three terms, it would **INFORMATION**. 
Entropy tells us the degree of information about an individual event. While, Cross-Entropy and KL divergence for more than one event. Two of very good explainantion that i found on internet are [post!](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/) and [video!](https://www.youtube.com/watch?v=ErfnhcEV1O8).