---
layout: single
title: Entropy, Cross-Entropy and  Kullbackâ€“Leibler (KL) divergence.
excerpt: "Intution without any complex math"
header:
  overlay_color: "#333"
share: true
toc: true

#date:   2019-1-12 16:16:01 -0600


---


** IN PROGRESS **

# Introduction
These three terms may sound different but they are related very closely to each other(on this later). If I have to choose a word which could explain all three terms, it would **INFORMATION**.  
Entropy tells us the degree of information about an individual event. While, Cross-Entropy and KL divergence for more than one event. Two of very good explainantion that i found on internet are [post!](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/) and [video!](https://www.youtube.com/watch?v=ErfnhcEV1O8).

## In-depth explainantion
In this section, I will be explaining each term and build on the previous one to connect the next.
#### Entropy
Entropy or I would say shanon entropy (as in information theory) is the average rate of information produced from a certain stochastic process. To understand this, let's start with an example.  
In Japan: A tremor occurs at least every five minutes, and each year there are up to 2,000 quakes that can be felt by people. Therefore, the chances of Japan(155) gettng stuck by an eartquake are very high compared to Israel(15) ([*list of countries by natural disaster risk*](https://en.wikipedia.org/wiki/List_of_countries_by_natural_disaster_risk)). Now if, somenone tells you that japan got hit by an earthquake you will be less surprised and therefore the amount of information is low compared to if israel hit by an earthquake you will be surprised and in return you are getting more information.