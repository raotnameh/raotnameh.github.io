---
layout: single
title: Automatic Speech Recognition(ASR) 
excerpt: "Basic Overview of Deep Learning Approach"
header:
  overlay_color: "#333"
share: false
toc: true

#date:   2019-1-7 16:16:01 -0600


---
The whole pipeline to convert speech to text is known as Automatic Speech Recognition (ASR), which formes the basis for Google's voice assistant sysytems. Let's dive deep into, how to build a basic one.  

In this article, you'll learn about the intuition behind the ASR. The article is divided into the following sections:

+ Introduction
+ ASR pipeline
+ Understanding Individual elements of ASR pipeline
+ Handling Out of Vocabulary Words (OOV) words  

# Introduction
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

Speech Recognition is a sequence labelling task, compared to image labelling where samples are independent. The fundamental equation of speech is  \\(P(w/o)\\) i.e. for given observations O = [o<sub>1</sub>,o<sub>2</sub>,.....,o<sub>n</sub>], we seek the most likley word sequence W = [w<sub>1</sub>,w<sub>2</sub>,.....,w<sub>n</sub>] \\[W = argmax (P(W / O)\\]

Like so many other tasks, [Bayes theorem ](https://en.wikipedia.org/wiki/Bayes%27_theorem) comes to our rescue. According to bayes theorem, \\(P(\frac{W}{O})\\) can be written as: \\[P(\frac{W}{O}) = \frac{P(\frac{O}{W}) * p(W)}{P(O)},\\]
\\(P(O)\\) is a marginalizing term or a constant for a particular given data. Therfore, \\[P(\frac{W}{O}) \\approx  P(\frac{O}{W}) * p(W).\\]  
<br />
* \\(P(O / W) \\): It means, given the observed **word sequence W**, the one Probability density function (PDF): \\(f(O / W;\\theta) \\) among all the PDF's that are most likely to have produced the **data O**. __\\(\\theta\\)__ is a learnable parameter and how we do that?? There are a lot of techquies to use, but I will be talking about Macimum Likelihood estimation or MLE (more on this later).
<br />
<br />
what do I mean by all the other PDF's? Let's take an example of gaussian distribution. why? because it will be easy for us in the math department. The gaussian distribution formualtion and it's plot can be seen in Fig. 1. As shown in fig for different values of \\(\\mu \\) and \\(\\sigma \\) we can get different plots or I would say different PDF's. therefore from each distribution we can get some data values which satisfies the given distribution or inversley, a specific set dataset can produce a PDF from a family of PDF's.

<p align='center'>
<img src="/assets/images/asr/gaussian.jpg">
<figcaption align='center'>Fig.1 Gaussian Distribution.</figcaption>
</p>

<br />

* \\(P(w)\\): It means, what is the probability of existing a particular word sequence. for example, the probability of a word sequence **I know of** is far greater than **I no of**. These probabilities can be calculated using a N-gram Toolkit or a Neural Language model.  
Popular N-gram language model toolkits are [KenLm](https://github.com/kpu/kenlm) and [SRILM](http://www.speech.sri.com/projects/srilm/).  

**----** <br />
Word error rate (**WER**), Character error rate (**CER**) and Sentence error rate (**SER**) are some of the metrices to _Measure the performance_ of ASR system.  

__WER__: It's computed using "[String edit distance](https://www.python-course.eu/levenshtein_distance.php)" or "[Levenstein distance](https://people.cs.pitt.edu/~kirk/cs1501/Pruhs/Spring2006/assignments/editdistance/Levenshtein%20Distance.htm)." It is,
<!-- \\[WER = \frac{N<sub>sub</sub> + N<sub>ins</sub> + N<sub>del</sub>}{N<sub>ref</sub>}\\]. Where,   -->

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
\begin{equation}
 WER = \dfrac{N_s + N_i + N_d}{N_r} 
\end{equation}

* N<sub>s</sub> = No. of substitutions, 
* N<sub>i</sub> = No. of insertions,
* N<sub>d</sub> = No. of deletions,
* N<sub>r</sub> = Words in the reference Transcription.  

i.e. the no of substitutions, insertions and deletions of __WORDS__ needed to get from the reference to hypothesis (ASR system best guess of the transcript).  

__CER__: Everythinng is same, except instead of WORDS it considers __CHARACTERS__ for all the calcualtions in the above formula after removing all the spaces in the sentence.  

__SER__: It's very simple to calculate. according tothe SER defination, even if a single word in a sentence is incorrect it wil label it as incorrect hypothesis. SER is used as _proportion of incorect sentences to the total number of sentences_. For example, if half of teh predicted sentences are incorrect, then the SER will be 50\%.



## ASR pipeline
ASR Pipeline contains mainly 2 main components:
* Acoustic Model and,
* Language Model (LM) <br />  

The acoustic model is responsible for calculating the \\(P(O / W) \\) and the LM calculates the \\(P(W) \\). Language model plays an important role, when there is ambguity in the predicted text. for example, **I *____* you**, if you have to choose between **_NO_** and **_KNOW_** which one whould you choose? Of course the second one, but why ? I would say, you have language understanding as one of your cognitive skills,that's how you  did it. Therefore, LM helps in making sense for a given word sequence.  <br /> 
<br />
let's look at a diagram of the whole pipeline, as shown in Fig. 2:
<p align='center'>
<img src="/assets/images/asr/asr_pipeline.png">
<figcaption align='center'>Fig.2 ASR Pipeline.</figcaption>
</p>


## Understanding Individual elements of ASR pipeline 
### Audio
Input is a __raw__ Audio signal/waveform, which is just the pressure waves recorded/stored digitally at a certain interval of time, also called a sampling rate/frequency as can be seen in  Fig. 3. To store an audio digitally, we sample it at some predefined sampling rate and a certain bit depth (8 or 16). The right fig shows a digital signal and left fig shows an analog signal.  

<p align='center'>
<img src="/assets/images/asr/audio.jpg">
<figcaption align='center'>Fig.3 Audio.</figcaption>
</p>

<font size="2"> _Just for the information, humans can hear sounds at frequencies from about 20 Hz to 20,000 Hz, though we hear sounds best from 1,000 Hz to 5,000 Hz, where human speech is centered._ </font>
### Spectrogram 
It's a better representation of an Audio signal to fed a Neural Network. It combines both time and frequency information. There is a saying in Machine learning communty: __Good Features make problems TRIVIAL__.
### Acoustic Model 
Acoustic models calculates/determines the Likelihood of output(Y) conditioned on an input(X) and the given parameters \\(\\theta\\) for a specific PDF which could have produced the input. The input is either Raw audio or spectrogram and the output is the predicted transcription.For each input Acoustic model ouputs Likelihood of a set of Word sequences i.e. \\(L(Y/X;\\theta)\\)
### Connectionist temporal classification (CTC) 
ctc is a scoring/loss function to calculate the loss of an continous ouput with any Alignment information given. It takes into account all the possible alignments possible for a given output sentence using a Heuristic approach.
### Decoder :
Decoder plays an important role in the ASR pipeline. It helps in getting a word error rate (WER) from 30-40 to under 15 or in some domain specific cases upto under 10 percent (%). The decoding process is of two types:
+ Without the Language model (LM)
+ With the Character level Language model (CLM)



## Handling OOV words
There are Two ways to handle out of vocabulary(OOV) words:
+ Use only the Acoustic model output without any Language Model (LM) and.
+ Use of Character level Language Model.

## References

+ [My paper](https://drive.google.com/open?id=18j58woXz5WUgkHaOO3b7byRWck5Oyzg7)

+ [Deepspeech1 paper](https://arxiv.org/pdf/1412.5567.pdf)

+ [Deepspeech2 paper](https://arxiv.org/pdf/1512.02595.pdf)

+ [CTC paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf)

+ [Spectrogram](https://en.wikipedia.org/wiki/Spectrogram)

If I have left someone, please let me know. I willadd the same to the refrence.

## My paper
<iframe src="/assets/images/asr/ASR_BigMM.pdf" width="700" height="800">