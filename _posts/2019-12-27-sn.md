---
layout: single
title: Smart Neural Networks a.k.a SmartNets.
excerpt: "It's something that will change how we use and train AI models."
header:
overlay_color: "#333"
share: true
toc: true

#date:   2019-12-27 16:17:01 -0600

---

*_IN PROGRESS_*
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

## Introduction
In this study, we try to address the problem that all the deep learning (DL) architectures fail to address i.e., a neural net taking exact number of operations on an input independent of the complexity of the input. We call our approach decisions with reasons or a SmartNet. Smart here refers to neural nets capability to decide when to use less features to make a decision.


## My views on the current state of AI
According to me, it all started with rules and still today we are finding rules ONLY. All that changed is NUMBER of rules. With the advancement in MACHINE LEARNING particularly DEEP LEARNING, we are now able to find thousand or million of rules (without any reasoning) instead of previously 5 to 10 hand crafted rules. 

Working on deep learning problems for the last three years, I have observed two things:

* Firstly, a neural network is a set of mathematical operations in a specific order (the layers). And if a single layer is removed (from the base network of an image classification neural net), the network is no better than randomly guessing the output classes. To prove my hypothesis, I did a little experiment on an image classification problem (using VGG-16). Indeed, it came out true as shown in Table below. After removing a single convolution layer, the accuracy dropped drastically, and the resulting reduced network is as good as randomly guessing a class from a given set of classes. This observation intrigued me a lot since it's a common understanding in the computer vision community that there's a kind of hierarchy, such that earlier layers learn low-level features, and the later layers learn high-level features. I was hoping, even after removing a layer, the model should still work (at least better than randomly guessing the output label), but it did not.

	| Metrics         | pre-trained     | fine-tuned | one layer removed |
	| --------------- | --------------- | ---------- | ----------------- |
	| Accuracy        | 13.052          | 88.667     | 8.936             |
	| Val-loss/image  | 2.316           | 0.562      | 89.225            |


* Secondly, as humans, in case of an image classification task, the time required to decide the output class depends on the complexity of the image. For a noisy image, we usually take longer to decide and vice versa, i.e., as opposed to a fixed number of operations in a neural network (or fixed time), humans follow different numbers of rules (variable time) depending on the complexity of an input image. 

If we look closely both observations are same. For example, If we solve the first problem, i.e., a neural model that can still classify an easy image even if a layer is removed and its deeper version for noisy images. To put it simply, only if we could "Build a neural model which can be subdivided into independent smaller functions to classify images depending on the complexity of input". According to me, building deeper models (black box) with billions of parameters is not the answer but a deeper understanding of the underlying model is. Furthermore, an explainable system would bolster the trust in AI systems. To the best of my knowledge, there is no previous work directly addressing the
problem in this direction. I have started working on this problem, and looking for collaborators who are interested to work on it with me.

**WE GOTTA PLAY SMART AND NOT HARD WITH BILLION OF PARAMTERS.**

## Dataset used
A subset of ImageNet dataset is used in this study (100 classes and the dataset is balanced with a train/val/test split of 1000/100/100 respectively.) with 3 different augmentations (rotation, horizontal flipping and noise). Furthermore, these 3 augmentation operations are sequentially applied recursively N times (N is randomly chosen between 1 to 10). For initial testing purposes, we only use 10 classes to decrease the training time for faster implentations of different architectures and methods

## How to solve it?
Can we have two different neural networks? one for extracting features and one to classify images based on the level of features. 
* Feature extractor: A CNN based model to extractor features having N layers in total. The layer at positino M-1 is indepent of the layers onwards and upto position N, but the layer at position N is dependent of all the previous layers. In mathematical terms, functions which are building on the previous funcitons and assiting them to learn features which are not learned before.
* Classifier: A model to classify the images based on the level of the feeatures. If we feed the features at the M-1 layer to the classifier, it's able to make deciions on the given features. And as we feed deeper layer the average accuracy of the classifier should increase. In mathematincal terms, a fucntion which can classify images based on the level of features as input.



This is what I have come up with for now. People could say it sounds similar to GANs, but if you ask me I would say not exactly. The basic difference is in GANs, the training is adversarial i.e., the networks try to outcompete each other. But in my proposed solution the feature extactor function has to learn the representations in a hierchal manner building on the previous layer (but with deeper features from later layers). To achieve this, the different feature extractor network layers have to work together. Or I can say they have to compete in a such a way that the average accuracy from the layers at the end should be greater than from the previous layers.

## Architecture

## References

## End
I hope you guys enjoyed this, please feel free to leave a comment or reach to me on **raotnameh@gmail.com**