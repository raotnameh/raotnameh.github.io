---
layout: single
title: Smart Neural Networks a.k.a SmartNets.
excerpt: "It's something that will change how we use and train AI models."
header:
overlay_color: "#333"
share: true
toc: true

#date:   2019-12-27 16:17:01 -0600

---

*_IN PROGRESS_*
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

## Introduction
In this study, we try to address the problem that all deep learning (DL) architectures fail to address i.e., a neural net taking exact number of operations on each and every input independent of the complexity of the input. We call our approach decisions with reasons or a SmartNet. Smart here refers to neural nets capability to decide when to use less features to make a decision.


### My views on the current state of AI
According to me, it all started with rules and still today we are finding rules ONLY. All that changed is NUMBER of rules. With advancement in MACHINE LEARNING particularly DEEP LEARNING, we are now finding thousand or million of rules instead of previously 5-10 hand crafted rules. 

**WE GOTTA PLAY SMART AND NOT HARD.................**

Working on deep learning problems for the last three years, I have realized two things:

* Firstly, a neural network is a set of mathematical operations in a specific order (the layers). And if a single layer is removed (from the base network of an image classification neural net), the network is no better than randomly guessing the output classes. To prove my hypothesis, I did a little experiment on an image classification problem (using VGG-16). Indeed, it came out true as shown in Table below. After removing a single convolution layer, the accuracy dropped drastically, and the resulting reduced network is as good as randomly guessing a class from a given set of classes. This observation intrigued me a lot since it's a common understanding in the computer vision community that there's a kind of hierarchy, such that earlier layers learn low-level features, and the later layers learn high-level features. I was hoping, even after removing a layer, the model should still work (at least better than randomly guessing the output label), but it did not.

| Metrics         | pre-trained     | fine-tuned | one layer removed |
| --------------- | --------------- | ---------- | ----------------- |
| Accuracy        | 13.052          | 88.667     | 8.936             |
| Val-loss/image  | 2.316           | 0.562      | 89.225            |


* Secondly, as humans, in case of an image classification task, the time required to decide the output class depends on the complexity of the image. For a noisy image, we usually take longer to decide and vice versa, i.e., as opposed to a fixed number of operations in a neural network (or fixed time), humans follow different numbers of rules (variable time) depending on the complexity of an input image. 

These two observations look different but are connected at the same time. For example, If we solve the first problem, i.e., a neural model that can still classify an easy image even if a layer is removed. And, use a deeper neural model to classify noisy images. To put it in other words, only if we could "Build a neural model which can be subdivided into smaller functions (depending on the complexity of input)". Building deeper models (black box) with billions of parameters is not the answer but a deeper understanding of the underlying model is. In addition, an explainable system would bolster the trust in AI systems. To the best of my knowledge, there is no previous work directly addressing the problem in this direction. If given an opportunity, I would like to work on a problem similar to this, but not limited to, and this is my potential proposed Ph.D. research problem statement.



## Dataset used
A subset of ImageNet dataset is used in this study (100 classes and the dataset is balanced) with 3 different augmentations (rotation, horizontal flipping and noise). Furthermore, these 3 augmentation operations are applied recursively N times (N is randomly chosen between 1 to 10). For initial testing purposes, we only use 10 classes to decrease the testing time for different architectures and methods

## How to solve it?
Can we have two different neural networks: one for a feature extracting and one is a classifier.

## References

## End
I hope you guys enjoyed this, please feel free to leave a comment or reach to me on **raotnameh@gmail.com**